{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "184ee9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 17:24:21,006 - INFO - Fetching raw data from hive.default.employees\n",
      "2025-09-08 17:24:21,297 - INFO - Connected to Trino with catalog hive\n",
      "2025-09-08 17:24:21,731 - INFO - Fetched 52 rows with Pandas from hive\n",
      "2025-09-08 17:24:21,796 - INFO - Created temporary Polars DataFrame with 52 rows and columns: ['name', 'city', 'salary', 'department', 'age', 'RecordID']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary DataFrame sample:\n",
      "shape: (5, 6)\n",
      "┌─────────┬───────────┬────────┬────────────┬─────┬──────────┐\n",
      "│ name    ┆ city      ┆ salary ┆ department ┆ age ┆ RecordID │\n",
      "│ ---     ┆ ---       ┆ ---    ┆ ---        ┆ --- ┆ ---      │\n",
      "│ str     ┆ str       ┆ i64    ┆ str        ┆ i64 ┆ i64      │\n",
      "╞═════════╪═══════════╪════════╪════════════╪═════╪══════════╡\n",
      "│ Alice   ┆ Mumbai    ┆ 55000  ┆ IT         ┆ 29  ┆ 1        │\n",
      "│ Bob     ┆ Delhi     ┆ 62000  ┆ HR         ┆ 35  ┆ 2        │\n",
      "│ Charlie ┆ Bangalore ┆ 70000  ┆ Finance    ┆ 40  ┆ 3        │\n",
      "│ Diana   ┆ Hyderabad ┆ 48000  ┆ IT         ┆ 27  ┆ 4        │\n",
      "│ Ethan   ┆ Chennai   ┆ 51000  ┆ Marketing  ┆ 30  ┆ 5        │\n",
      "└─────────┴───────────┴────────┴────────────┴─────┴──────────┘\n",
      "\n",
      "DataFrame schema:\n",
      "Schema([('name', String), ('city', String), ('salary', Int64), ('department', String), ('age', Int64), ('RecordID', Int64)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "import trino\n",
    "import gc\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Trino Connection Configuration (dynamic for multiple catalogs)\n",
    "TRINO_HOST = \"3.108.199.0\"\n",
    "TRINO_PORT = 32092\n",
    "TRINO_USER = \"root\"\n",
    "TRINO_CATALOG = \"hive\"  # Change to hive, mongo, pinot, postgresql as needed\n",
    "TRINO_SCHEMA = \"default\"      # Update per catalog (e.g., default, db, public)\n",
    "TRINO_TABLE = \"employees\"  # Update per catalog (e.g., collection for mongo)\n",
    "\n",
    "# Construct SQLAlchemy connection string\n",
    "conn_str = f\"trino://{TRINO_USER}@{TRINO_HOST}:{TRINO_PORT}/{TRINO_CATALOG}/{TRINO_SCHEMA}\"\n",
    "\n",
    "def get_trino_connection():\n",
    "    \"\"\"Create a SQLAlchemy engine for Trino.\"\"\"\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            conn_str,\n",
    "            connect_args={'http_scheme': 'http', 'auth': None}  # Use 'https' if SSL enabled\n",
    "        )\n",
    "        logging.info(f\"Connected to Trino with catalog {TRINO_CATALOG}\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create Trino engine: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Simplified query to fetch raw data (no SQL sanitization to avoid Trino issues)\n",
    "logging.info(f\"Fetching raw data from {TRINO_CATALOG}.{TRINO_SCHEMA}.{TRINO_TABLE}\")\n",
    "base_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM {TRINO_CATALOG}.{TRINO_SCHEMA}.{TRINO_TABLE}\n",
    "LIMIT 1000000  -- Remove for full dataset; keep for testing\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Primary attempt: Use Pandas + SQLAlchemy for robust fetching\n",
    "    engine = get_trino_connection()\n",
    "    # Chunked reading for scalability\n",
    "    df_pandas_chunks = pd.read_sql(text(base_query), engine, chunksize=100000)\n",
    "    df_pandas = pd.concat([chunk for chunk in df_pandas_chunks], ignore_index=True)\n",
    "    logging.info(f\"Fetched {len(df_pandas)} rows with Pandas from {TRINO_CATALOG}\")\n",
    "    \n",
    "    # Add RecordID if not present\n",
    "    if 'RecordID' not in df_pandas.columns:\n",
    "        df_pandas['RecordID'] = range(1, len(df_pandas) + 1)\n",
    "    \n",
    "    # Sanitize dirty data in Pandas (preserves original table)\n",
    "    string_cols = df_pandas.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        if col != 'RecordID':\n",
    "            df_pandas[col] = df_pandas[col].astype(str).str.strip().str.replace(r'[\\n\\r\\t%]', ' ', regex=True)\n",
    "            df_pandas[col] = df_pandas[col].replace({'': None, 'nan': None, 'None': None})\n",
    "    \n",
    "    # Handle DOB as date if present\n",
    "    if 'dob' in df_pandas.columns:\n",
    "        df_pandas['dob'] = pd.to_datetime(df_pandas['dob'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    # Convert to Polars for fast processing\n",
    "    df = pl.from_pandas(df_pandas)\n",
    "    logging.info(f\"Created temporary Polars DataFrame with {len(df)} rows and columns: {df.columns}\")\n",
    "    \n",
    "    # Display sample and schema\n",
    "    print(\"Temporary DataFrame sample:\")\n",
    "    print(df.head(5))\n",
    "    print(\"\\nDataFrame schema:\")\n",
    "    print(df.schema)\n",
    "    \n",
    "    # Clean up\n",
    "    engine.dispose()\n",
    "    del df_pandas\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Primary fetch failed: {str(e)}\")\n",
    "    # Fallback: Use raw Trino DBAPI\n",
    "    try:\n",
    "        conn = trino.dbapi.connect(\n",
    "            host=TRINO_HOST,\n",
    "            port=TRINO_PORT,\n",
    "            user=TRINO_USER,\n",
    "            catalog=TRINO_CATALOG,\n",
    "            schema=TRINO_SCHEMA,\n",
    "            http_scheme='http'  # Use 'https' if SSL enabled\n",
    "        )\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(base_query)\n",
    "        rows = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]\n",
    "        df_pandas = pd.DataFrame(rows, columns=columns)\n",
    "        logging.info(f\"Fallback fetched {len(df_pandas)} rows with Trino DBAPI\")\n",
    "        \n",
    "        # Add RecordID and sanitize (same as primary path)\n",
    "        if 'RecordID' not in df_pandas.columns:\n",
    "            df_pandas['RecordID'] = range(1, len(df_pandas) + 1)\n",
    "        string_cols = df_pandas.select_dtypes(include=['object']).columns\n",
    "        for col in string_cols:\n",
    "            if col != 'RecordID':\n",
    "                df_pandas[col] = df_pandas[col].astype(str).str.strip().str.replace(r'[\\n\\r\\t%]', ' ', regex=True)\n",
    "                df_pandas[col] = df_pandas[col].replace({'': None, 'nan': None, 'None': None})\n",
    "        if 'dob' in df_pandas.columns:\n",
    "            df_pandas['dob'] = pd.to_datetime(df_pandas['dob'], errors='coerce', dayfirst=True)\n",
    "        \n",
    "        # Convert to Polars\n",
    "        df = pl.from_pandas(df_pandas)\n",
    "        logging.info(f\"Fallback created Polars DataFrame with {len(df)} rows and columns: {df.columns}\")\n",
    "        \n",
    "        # Display sample and schema\n",
    "        print(\"Temporary DataFrame sample (fallback):\")\n",
    "        print(df.head(5))\n",
    "        print(\"\\nDataFrame schema (fallback):\")\n",
    "        print(df.schema)\n",
    "        \n",
    "        conn.close()\n",
    "        del df_pandas\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as fallback_e:\n",
    "        logging.error(f\"Fallback failed: {str(fallback_e)}\")\n",
    "        raise ValueError(f\"Cannot fetch data from {TRINO_CATALOG}. Check connection, permissions, or table existence. Full error: {str(fallback_e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2076071a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 17:24:25,356 - INFO - BlockingFactory.auto_create called with df=True, conn_str=None, view_name=None, collection=None\n",
      "2025-09-08 17:24:25,372 - INFO - Creating PolarsBlocking\n",
      "2025-09-08 17:24:25,372 - INFO - Initializing PolarsBlocking with DataFrame columns: ['name', 'city', 'salary', 'department', 'age', 'RecordID']\n",
      "2025-09-08 17:24:25,372 - INFO - Attribute map: {'customer_name': 'name', 'city': 'city'}\n",
      "2025-09-08 17:24:25,372 - INFO - Generated rules: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blocking initialized\n",
      "Blocking rules available: []\n"
     ]
    }
   ],
   "source": [
    "from universal_blocking import BlockingFactory\n",
    "try:\n",
    "         blocker = BlockingFactory.auto_create(\n",
    "             df=df,\n",
    "             record_id_col=\"RecordID\"\n",
    "         )\n",
    "         print(\"✅ Blocking initialized\")\n",
    "         print(f\"Blocking rules available: {list(blocker.rules.keys())}\")\n",
    "except Exception as e:\n",
    "        logging.error(f\"Failed to initialize blocking: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8ebd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 17:24:25,410 - INFO - DataFrame columns and types: {'name': String, 'city': String, 'salary': Int64, 'department': String, 'age': Int64, 'RecordID': Int64}\n",
      "2025-09-08 17:24:25,411 - INFO - Attribute map: {'customer_name': 'name', 'city': 'city'}\n",
      "2025-09-08 17:24:25,412 - INFO - Running all rules (parallel=True, max_workers=8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No candidate pairs found.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import altair as alt\n",
    "from IPython.display import display\n",
    "\n",
    "# Ensure logging is configured\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Global variable to store per_rule_dfs for Cell 4\n",
    "global per_rule_dfs\n",
    "\n",
    "try:\n",
    "    logging.info(f\"DataFrame columns and types: {dict(zip(df.columns, df.dtypes))}\")\n",
    "    logging.info(f\"Attribute map: {blocker.attr_map}\")\n",
    "    \n",
    "    per_rule_dfs = blocker.run_all(parallel=True, max_workers=8)\n",
    "    \n",
    "    if per_rule_dfs:\n",
    "        stats_df, _ = blocker.generate_rule_report(\n",
    "            per_rule_dfs=per_rule_dfs,\n",
    "            show_top_n=10,\n",
    "            save_html_path=None\n",
    "        )\n",
    "        chart = (\n",
    "            alt.Chart(stats_df)\n",
    "            .mark_bar()\n",
    "            .encode(\n",
    "                x=alt.X(\"Rule:N\", sort=\"-y\", title=\"Blocking Rule\"),\n",
    "                y=alt.Y(\"PairsPct:Q\", title=\"Candidate Pairs (%)\"),\n",
    "                tooltip=[\n",
    "                    alt.Tooltip(\"Rule:N\", title=\"Rule\"),\n",
    "                    alt.Tooltip(\"Pairs:Q\", format=\",.0f\", title=\"Pairs\"),\n",
    "                    alt.Tooltip(\"PairsPct:Q\", format=\".2f\", title=\"Pairs (%)\"),\n",
    "                    alt.Tooltip(\"UniqueRecords:Q\", format=\",.0f\", title=\"Unique Records\"),\n",
    "                    alt.Tooltip(\"AvgBlockSize:Q\", format=\".2f\", title=\"Avg Block Size\")\n",
    "                ],\n",
    "                color=alt.Color(\"PairsPct:Q\", scale=alt.Scale(scheme=\"blues\"), title=\"Pairs (%)\")\n",
    "            )\n",
    "            .properties(\n",
    "                title=\"Blocking Rules by Percentage of Candidate Pairs\",\n",
    "                width=600,\n",
    "                height=400\n",
    "            )\n",
    "        )\n",
    "        print(\"✅ Blocking rules executed and report generated\")\n",
    "        display(stats_df)\n",
    "        display(chart)\n",
    "    else:\n",
    "        print(\"No candidate pairs found.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to run blocking rules or generate report: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b28a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 17:24:25,457 - INFO - Memory before merging: 201.14 MB\n",
      "2025-09-08 17:24:25,460 - INFO - No pairs to merge\n",
      "2025-09-08 17:24:25,747 - INFO - Memory after cleanup: 202.22 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No candidate pairs to cluster\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import altair as alt\n",
    "from IPython.display import display\n",
    "import polars as pl\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Ensure logging is configured\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def merge_pairs(per_rule_dfs: dict) -> pl.DataFrame:\n",
    "    \"\"\"Merge candidate pairs from per_rule_dfs, avoiding re-running rules.\"\"\"\n",
    "    if not per_rule_dfs:\n",
    "        logging.info(\"No pairs to merge\")\n",
    "        return pl.DataFrame(schema=[\"RecordID1\", \"RecordID2\", \"RulesUsed\"])\n",
    "    logging.info(\"Merging candidate pairs\")\n",
    "    combined = pl.concat(list(per_rule_dfs.values()))\n",
    "    agg = combined.group_by([\"RecordID1\", \"RecordID2\"]).agg(\n",
    "        RulesUsed=pl.col(\"Rule\").unique().sort().str.join(\",\")\n",
    "    )\n",
    "    logging.info(f\"Merged {len(agg)} unique pairs\")\n",
    "    return agg\n",
    "\n",
    "try:\n",
    "    # Log memory usage\n",
    "    process = psutil.Process()\n",
    "    logging.info(f\"Memory before merging: {process.memory_info().rss / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Reuse per_rule_dfs from Cell 3\n",
    "    if 'per_rule_dfs' not in globals():\n",
    "        raise ValueError(\"per_rule_dfs not found. Run Cell 3 first.\")\n",
    "    merged_pairs = merge_pairs(per_rule_dfs)\n",
    "    \n",
    "    # Create clusters in chunks\n",
    "    if not merged_pairs.is_empty():\n",
    "        logging.info(f\"Processing {len(merged_pairs)} pairs for clustering\")\n",
    "        G = nx.Graph()\n",
    "        chunk_size = 100000  # Reduced to 100K pairs per chunk\n",
    "        for i in range(0, len(merged_pairs), chunk_size):\n",
    "            chunk = merged_pairs[i:i + chunk_size]\n",
    "            logging.info(f\"Adding chunk {i // chunk_size + 1} with {len(chunk)} pairs\")\n",
    "            for row in chunk.iter_rows(named=True):\n",
    "                G.add_edge(row[\"RecordID1\"], row[\"RecordID2\"], rules=row[\"RulesUsed\"])\n",
    "            # Clear memory\n",
    "            del chunk\n",
    "            gc.collect()\n",
    "            logging.info(f\"Memory after chunk {i // chunk_size + 1}: {process.memory_info().rss / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Generate clusters\n",
    "        logging.info(\"Generating clusters\")\n",
    "        clusters = []\n",
    "        min_cluster_size = 2\n",
    "        for i, component in enumerate(nx.connected_components(G)):\n",
    "            if len(component) >= min_cluster_size:\n",
    "                for record_id in component:\n",
    "                    clusters.append({\"ClusterID\": i + 1, \"RecordID\": record_id})\n",
    "        \n",
    "        clusters_df = pd.DataFrame(clusters)\n",
    "        if not clusters_df.empty:\n",
    "            # Create Altair chart for cluster size distribution\n",
    "            cluster_sizes = clusters_df.groupby(\"ClusterID\").size().reset_index(name=\"Size\")\n",
    "            chart = (\n",
    "                alt.Chart(cluster_sizes)\n",
    "                .mark_bar()\n",
    "                .encode(\n",
    "                    x=alt.X(\"Size:Q\", bin=alt.Bin(maxbins=20), title=\"Cluster Size (Records)\"),\n",
    "                    y=alt.Y(\"count():Q\", title=\"Number of Clusters\"),\n",
    "                    tooltip=[\n",
    "                        alt.Tooltip(\"Size:Q\", title=\"Cluster Size\"),\n",
    "                        alt.Tooltip(\"count():Q\", title=\"Number of Clusters\")\n",
    "                    ],\n",
    "                    color=alt.Color(\"count():Q\", scale=alt.Scale(scheme=\"greens\"), title=\"Cluster Count\")\n",
    "                )\n",
    "                .properties(\n",
    "                    title=\"Distribution of Cluster Sizes\",\n",
    "                    width=600,\n",
    "                    height=400\n",
    "                )\n",
    "            )\n",
    "            print(f\"✅ Created {len(clusters_df['ClusterID'].unique())} clusters with {len(clusters_df)} records\")\n",
    "            display(clusters_df)  # Display cluster DataFrame\n",
    "            display(chart)        # Display Altair chart\n",
    "        else:\n",
    "            print(\"No clusters found with minimum size of 2\")\n",
    "    else:\n",
    "        print(\"No candidate pairs to cluster\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to merge pairs or create clusters: {str(e)}\")\n",
    "    raise\n",
    "finally:\n",
    "    # Clean up memory\n",
    "    if 'G' in locals():\n",
    "        del G\n",
    "    if 'merged_pairs' in locals():\n",
    "        del merged_pairs\n",
    "    gc.collect()\n",
    "    logging.info(f\"Memory after cleanup: {process.memory_info().rss / 1024**2:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
