{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install splink pyspark rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfc7c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from splink.backends.spark import similarity_jar_location\n",
    "from splink import SparkAPI\n",
    "\n",
    "# Path to your custom JAR file\n",
    "CUSTOM_JAR_PATH = r\"C:\\Users\\AbhayPandey\\Desktop\\AP_SS\\Note\\scala-udf-similarity-0.1.1-shaded.jar\"\n",
    "\n",
    "# --- Use the modern SparkSession.builder pattern for configuration ---\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"SplinkRobustMemory\")\n",
    "    .config(\"spark.driver.memory\", \"12g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "\n",
    "    # --- THE CRITICAL FIX FOR THIS ERROR ---\n",
    "    # This setting explicitly allocates more memory to the Python processes\n",
    "    # that run your UDFs, preventing them from being killed for using too much RAM.\n",
    "    .config(\"spark.python.worker.memory\", \"4g\")\n",
    "\n",
    "    # --- Other robust settings for stability ---\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") # Safety for actions like .show()\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"true\") # Important for performance\n",
    "    .config(\"spark.ui.port\", \"4040\") # For monitoring\n",
    "\n",
    "    # Jars and Packages\n",
    "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.2-s_2.12\")\n",
    "    .config(\"spark.jars\", f\"{similarity_jar_location()},{CUSTOM_JAR_PATH}\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Set checkpoint directory\n",
    "spark.sparkContext.setCheckpointDir(\"./tmp_checkpoints\")\n",
    "\n",
    "print(\"✅ Spark Session created with robust memory settings.\")\n",
    "print(f\"Access the Spark UI at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184cd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, DoubleType, ArrayType\n",
    "# from pyspark.sql import callUDF\n",
    "\n",
    "\n",
    "# Phonetic / normalization\n",
    "spark.udf.registerJavaFunction(\"accent_remove\", \"uk.gov.moj.dash.linkage.AccentRemover\", StringType())\n",
    "spark.udf.registerJavaFunction(\"double_metaphone\", \"uk.gov.moj.dash.linkage.DoubleMetaphone\", StringType())\n",
    "spark.udf.registerJavaFunction(\"double_metaphone_alt\", \"uk.gov.moj.dash.linkage.DoubleMetaphoneAlt\", StringType())\n",
    "\n",
    "# Similarity\n",
    "spark.udf.registerJavaFunction(\"cosine_distance\", \"uk.gov.moj.dash.linkage.CosineDistance\", DoubleType())\n",
    "spark.udf.registerJavaFunction(\"jaccard_similarity\", \"uk.gov.moj.dash.linkage.JaccardSimilarity\", DoubleType())\n",
    "spark.udf.registerJavaFunction(\"jaro_similarity\", \"uk.gov.moj.dash.linkage.JaroSimilarity\", DoubleType())\n",
    "spark.udf.registerJavaFunction(\"jaro_winkler_similarity\", \"uk.gov.moj.dash.linkage.JaroWinklerSimilarity\", DoubleType())\n",
    "spark.udf.registerJavaFunction(\"lev_damerau_distance\", \"uk.gov.moj.dash.linkage.LevDamerauDistance\", DoubleType())\n",
    "\n",
    "# Tokenisers\n",
    "spark.udf.registerJavaFunction(\"qgram_tokeniser\", \"uk.gov.moj.dash.linkage.QgramTokeniser\", StringType())\n",
    "spark.udf.registerJavaFunction(\"q2gram_tokeniser\", \"uk.gov.moj.dash.linkage.Q2gramTokeniser\", StringType())\n",
    "spark.udf.registerJavaFunction(\"q3gram_tokeniser\", \"uk.gov.moj.dash.linkage.Q3gramTokeniser\", StringType())\n",
    "spark.udf.registerJavaFunction(\"q4gram_tokeniser\", \"uk.gov.moj.dash.linkage.Q4gramTokeniser\", StringType())\n",
    "spark.udf.registerJavaFunction(\"q5gram_tokeniser\", \"uk.gov.moj.dash.linkage.Q5gramTokeniser\", StringType())\n",
    "spark.udf.registerJavaFunction(\"q6gram_tokeniser\", \"uk.gov.moj.dash.linkage.Q6gramTokeniser\", StringType())\n",
    "\n",
    "# Array / explode helpers\n",
    "spark.udf.registerJavaFunction(\"dual_array_explode\", \"uk.gov.moj.dash.linkage.DualArrayExplode\", ArrayType(StringType()))\n",
    "spark.udf.registerJavaFunction(\"latlong_explode\", \"uk.gov.moj.dash.linkage.latlongexplode\", ArrayType(StringType()))\n",
    "\n",
    "# Escaping\n",
    "spark.udf.registerJavaFunction(\"sql_escape\", \"uk.gov.moj.dash.linkage.sqlEscape\", StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78170a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "csv_path = \"data.csv\"   # replace with your dataset\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "df = df.withColumn(\"unique_id\", monotonically_increasing_id().cast(StringType()))\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"Showing first 5 rows:\")\n",
    "df.show(5, truncate=False) # truncate=False makes it easier to see full column content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil, glob, os\n",
    "\n",
    "# def move_single_csv(folder_path, final_csv):\n",
    "#     \"\"\"Move Spark's part-0000.csv to a single clean CSV file.\"\"\"\n",
    "#     part_file = glob.glob(os.path.join(folder_path, \"part-*.csv\"))[0]\n",
    "#     shutil.move(part_file, final_csv)\n",
    "#     shutil.rmtree(folder_path)  # cleanup temp folder\n",
    "\n",
    "\n",
    "# def generate_predictions(linker, prediction_path: str, cluster_path: str, threshold: float):\n",
    "#     # Predictions\n",
    "#     df_predictions = linker.inference.predict()\n",
    "#     df_predictions_spark = df_predictions.as_spark_dataframe()\n",
    "#     df_predictions_spark = df_predictions_spark.filter(df_predictions_spark.match_probability > threshold)\n",
    "\n",
    "#     (df_predictions_spark\n",
    "#         .coalesce(1)\n",
    "#         .write.mode(\"overwrite\")\n",
    "#         .option(\"header\", True)\n",
    "#         .csv(prediction_path.replace(\".csv\",\"\")))\n",
    "\n",
    "#     # Clusters\n",
    "#     clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n",
    "#         df_predictions, threshold_match_probability=threshold\n",
    "#     )\n",
    "#     clusters_spark = clusters.as_spark_dataframe()\n",
    "\n",
    "#     (clusters_spark\n",
    "#         .coalesce(1)\n",
    "#         .write.mode(\"overwrite\")\n",
    "#         .option(\"header\", True)\n",
    "#         .csv(cluster_path.replace(\".csv\",\"\")))\n",
    "\n",
    "#     # Rename temp folders to final CSVs\n",
    "#     move_single_csv(prediction_path.replace(\".csv\",\"\"), prediction_path)\n",
    "#     move_single_csv(cluster_path.replace(\".csv\",\"\"), cluster_path)\n",
    "\n",
    "#     return df_predictions_spark, clusters_spark\n",
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "def move_single_csv(folder_path, final_csv):\n",
    "    \"\"\"\n",
    "    Finds Spark's output part-file and renames it to the desired final CSV name.\n",
    "    \"\"\"\n",
    "    part_files = glob.glob(os.path.join(folder_path, \"part-*.csv\"))\n",
    "    if not part_files:\n",
    "        print(f\"Warning: No data was written to the output folder {folder_path}.\")\n",
    "        with open(final_csv, 'w', newline='') as f:\n",
    "            pass\n",
    "        if os.path.exists(folder_path):\n",
    "            shutil.rmtree(folder_path)\n",
    "        return\n",
    "\n",
    "    part_file = part_files[0]\n",
    "    shutil.move(part_file, final_csv)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "\n",
    "\n",
    "def generate_predictions_single_csv(\n",
    "    linker,\n",
    "    original_df_spark: SparkDataFrame,\n",
    "    prediction_path: str,\n",
    "    cluster_path: str,\n",
    "    threshold: float\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates predictions and a COMPLETE, SORTED cluster file as a SINGLE CSV.\n",
    "    \"\"\"\n",
    "    # (Prediction logic remains the same)\n",
    "    start_time = time.time()\n",
    "    df_predictions = linker.inference.predict()\n",
    "    df_predictions_spark = df_predictions.as_spark_dataframe()\n",
    "    df_predictions_spark = df_predictions_spark.filter(f\"match_probability > {threshold}\")\n",
    "\n",
    "    temp_prediction_folder = prediction_path + \"_temp\"\n",
    "    (df_predictions_spark\n",
    "        .coalesce(1)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(temp_prediction_folder))\n",
    "\n",
    "    move_single_csv(temp_prediction_folder, prediction_path)\n",
    "    print(f\"Single prediction CSV written to '{prediction_path}' in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "    # (Cluster generation logic)\n",
    "    start_time = time.time()\n",
    "    clusters = linker.clustering.cluster_pairwise_predictions_at_threshold(\n",
    "        df_predictions, threshold_match_probability=threshold\n",
    "    )\n",
    "    clusters_spark_map = clusters.as_spark_dataframe()\n",
    "\n",
    "    # --- FIX: Select only essential columns to prevent duplicate column errors ---\n",
    "    # This ensures no extra columns like 'address_line1' are carried into the join.\n",
    "    clusters_spark_map = clusters_spark_map.select(\"unique_id\", \"cluster_id\")\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    # CRITICAL LOGIC: JOIN CLUSTERS BACK TO THE ORIGINAL DATA\n",
    "    # This join is now safe because clusters_spark_map only has unique_id and cluster_id.\n",
    "    full_df_with_clusters = original_df_spark.join(\n",
    "        clusters_spark_map, on=\"unique_id\", how=\"left\"\n",
    "    )\n",
    "    full_df_with_clusters = full_df_with_clusters.withColumn(\n",
    "        \"cluster_id\", coalesce(col(\"cluster_id\"), col(\"unique_id\"))\n",
    "    )\n",
    "    full_df_with_clusters = full_df_with_clusters.sort(\"cluster_id\", \"unique_id\")\n",
    "\n",
    "    # (Writing logic remains the same)\n",
    "    temp_cluster_folder = cluster_path + \"_temp\"\n",
    "    (full_df_with_clusters\n",
    "        .coalesce(1)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(temp_cluster_folder))\n",
    "\n",
    "    move_single_csv(temp_cluster_folder, cluster_path)\n",
    "    print(f\"Single, full cluster CSV written to '{cluster_path}' in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "    return df_predictions_spark, full_df_with_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30dd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import auto_blocking2 as ab\n",
    "# from splink import SparkAPI, Linker\n",
    "\n",
    "# db_api = SparkAPI(spark_session=spark)\n",
    "# settings, roles, diagnostics, df_enhanced = ab.auto_generate_settings(df, db_api, spark=spark)  # Add spark=spark here\n",
    "# print(\"Roles:\", roles)\n",
    "# print(\"Diagnostics:\", diagnostics)\n",
    "\n",
    "# # Sample 40% of the data for training\n",
    "# training_df = df_enhanced.sample(0.4, seed=42)\n",
    "\n",
    "# # Initialize Linker with training data and train\n",
    "# linker = Linker(training_df, settings, db_api=db_api)\n",
    "# deterministic_rules = [diag['rule'] for diag in diagnostics if diag['kept']]\n",
    "# try:\n",
    "#     linker.training.estimate_probability_two_random_records_match(\n",
    "#         deterministic_matching_rules=deterministic_rules,\n",
    "#         recall=0.95\n",
    "#     )\n",
    "# except:\n",
    "#     linker.training.estimate_probability_two_random_records_match(\n",
    "#         deterministic_matching_rules=deterministic_rules,\n",
    "#         recall=1.0\n",
    "#     )\n",
    "# linker.training.estimate_u_using_random_sampling(max_pairs=5e5)\n",
    "# if deterministic_rules:\n",
    "#     linker.training.estimate_parameters_using_expectation_maximisation(deterministic_rules[0])\n",
    "\n",
    "# # Save the trained model to JSON\n",
    "# linker.misc.save_model_to_json(\"splink_model.json\", overwrite=True)\n",
    "\n",
    "# # Load the trained model with the full DataFrame\n",
    "# import json\n",
    "# with open(\"splink_model.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "#     trained_settings = json.load(fh)\n",
    "# full_linker = Linker(df_enhanced, trained_settings, db_api=db_api)\n",
    "\n",
    "# df_preds_spark, clusters_spark = generate_predictions(\n",
    "#     full_linker,\n",
    "#     \"splink_predictions.csv\",\n",
    "#     \"splink_clusters.csv\",\n",
    "#     threshold=0.99\n",
    "# )    \n",
    "\n",
    "import auto_blocking2 as ab\n",
    "from splink import SparkAPI, Linker\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- Setup (same as before) ---\n",
    "db_api = SparkAPI(spark_session=spark)\n",
    "\n",
    "# --- 1. Generate Settings (The slow UDFs run here) ---\n",
    "start_time = time.time()\n",
    "settings, roles, diagnostics, df_enhanced = ab.auto_generate_settings(df, db_api, spark=spark)\n",
    "print(f\"✅ Settings and derived columns generated in {time.time() - start_time:.2f} seconds.\")\n",
    "print(\"\\nDetected Roles:\", roles)\n",
    "print(\"\\nBlocking Rule Diagnostics:\", diagnostics)\n",
    "\n",
    "# --- 2. OPTIMIZATION: Cache the enhanced dataframe ---\n",
    "# This is the most important change. It stores the result of the slow UDFs in memory.\n",
    "# All subsequent steps will be much faster because they read from this cache.\n",
    "df_enhanced.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "# This action triggers the computation and caching.\n",
    "print(f\"\\nCached enhanced DataFrame with {df_enhanced.count()} rows.\")\n",
    "\n",
    "\n",
    "# --- 3. MODIFIED: Use a 40% sample for training ---\n",
    "# The .sample() function takes a fraction (0.4 = 40%).\n",
    "# `seed=42` ensures you get the same random sample every time you run the code.\n",
    "training_df = df_enhanced.sample(0.4, seed=42)\n",
    "\n",
    "# Cache the training set as it will be used repeatedly during the training steps.\n",
    "training_df.cache()\n",
    "print(f\"✅ Using a 40% sample of {training_df.count()} records for model training.\")\n",
    "# --- 4. Train the Model (now on the small, fast sample) ---\n",
    "start_time = time.time()\n",
    "linker = Linker(training_df, settings, db_api=db_api)\n",
    "deterministic_rules = [diag['rule'] for diag in diagnostics if diag['kept']]\n",
    "try:\n",
    "    linker.training.estimate_probability_two_random_records_match(\n",
    "        deterministic_matching_rules=deterministic_rules, recall=0.95\n",
    "    )\n",
    "except Exception:\n",
    "    linker.training.estimate_probability_two_random_records_match(\n",
    "        deterministic_matching_rules=deterministic_rules, recall=1.0\n",
    "    )\n",
    "linker.training.estimate_u_using_random_sampling(max_pairs=2e6)\n",
    "\n",
    "# --- MODIFICATION START: Train the model on ALL good blocking rules ---\n",
    "# This loop teaches the model from a much richer set of examples.\n",
    "print(\"\\nStarting Expectation Maximisation training on all blocking rules combined...\")\n",
    "linker.training.estimate_parameters_using_expectation_maximisation(deterministic_rules)\n",
    "# Save the trained model\n",
    "linker.misc.save_model_to_json(\"splink_model.json\", overwrite=True)\n",
    "print(f\"✅ Model training completed in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "# Clean up the training cache\n",
    "training_df.unpersist()\n",
    "\n",
    "# --- 5. Predict on the FULL dataset ---\n",
    "# Load the trained model\n",
    "with open(\"splink_model.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "    trained_settings = json.load(fh)\n",
    "# Create a new linker with the FULL (cached) enhanced dataframe\n",
    "full_linker = Linker(df_enhanced, trained_settings, db_api=db_api)\n",
    "\n",
    "# --- 6. CRITICAL CHANGE: Call the correct function from the previous cell ---\n",
    "# Use the function that generates a single, complete CSV file.\n",
    "# We must pass `df_enhanced` so it can join the full record data.\n",
    "df_preds_spark, clusters_spark_full = generate_predictions_single_csv(\n",
    "    full_linker,\n",
    "    df_enhanced,  # <-- Pass the full, cached data here\n",
    "    \"splink_predictions.csv\",\n",
    "    \"splink_clusters.csv\",\n",
    "    threshold=0.95\n",
    ")\n",
    "\n",
    "# --- 7. Clean up the main cache ---\n",
    "df_enhanced.unpersist()\n",
    "\n",
    "# --- 8. Show Final Results ---\n",
    "print(\"\\n✅ Job Complete! Final files have been generated.\")\n",
    "print(\"Showing a sample of the final, complete cluster data:\")\n",
    "clusters_spark_full.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "477ecc3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~plink (C:\\Users\\AbhayPandey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
